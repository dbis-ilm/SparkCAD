{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e6529b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "import graphviz\n",
    "import pandas as pd\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from copy import copy\n",
    "import re\n",
    "import configparser\n",
    "from pathlib import Path\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "\n",
    "class RddAsNode:\n",
    "    \n",
    "    def __init__(self, name, is_cached, number_of_usage, number_of_computations):\n",
    "        self.name = name\n",
    "        self.is_cached = is_cached\n",
    "        self.number_of_usage = number_of_usage\n",
    "        self.number_of_computations = number_of_computations\n",
    "\n",
    "        \n",
    "class Rdd:\n",
    "    \n",
    "    def __init__(self, id, name, parents_lst, stage_id, job_id, is_cached):\n",
    "        self.id = id\n",
    "        self.name = name\n",
    "        self.parents_lst = parents_lst\n",
    "        self.stage_id = stage_id\n",
    "        self.job_id = job_id\n",
    "        self.is_cached = is_cached\n",
    "\n",
    "        \n",
    "class Transformation:\n",
    "    \n",
    "    def __init__(self, from_rdd, to_rdd, is_narrow):\n",
    "        self.from_rdd = from_rdd\n",
    "        self.to_rdd = to_rdd\n",
    "        self.is_narrow = is_narrow\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        if (isinstance(other, Transformation)):\n",
    "            return self.from_rdd == other.from_rdd and self.to_rdd == other.to_rdd\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.from_rdd) ^ hash(self.to_rdd)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if self.from_rdd == other.from_rdd:\n",
    "            self.to_rdd < other.to_rdd\n",
    "        return self.from_rdd < other.from_rdd\n",
    "\n",
    "    \n",
    "class CachingPlanItem:\n",
    "    \n",
    "    def __init__(self, stage_id, job_id, rdd_id, is_cache_item):\n",
    "        self.stage_id = stage_id\n",
    "        self.job_id = job_id\n",
    "        self.rdd_id = rdd_id\n",
    "        self.is_cache_item = is_cache_item\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        if self.job_id == other.job_id:\n",
    "            if self.stage_id == other.stage_id:\n",
    "                if self.is_cache_item == other.is_cache_item:\n",
    "                    return self.rdd_id\n",
    "                return self.is_cache_item\n",
    "            return self.stage_id < other.stage_id\n",
    "        return self.job_id < other.job_id\n",
    "\n",
    "\n",
    "class Utility():\n",
    "    def get_absolute_path(path):\n",
    "        if not os.path.isabs(path):\n",
    "            return str(Path().absolute()) + '/' + path\n",
    "        return path\n",
    "\n",
    "    def intersection(lst1, lst2):\n",
    "        lst3 = [value for value in lst1 if value in lst2]\n",
    "        return lst3    \n",
    "\n",
    "    \n",
    "class FactHub():\n",
    "    \n",
    "    app_name = \"\"\n",
    "    job_info_dect = {}\n",
    "    stage_info_dect = {}\n",
    "    stage_job_dect = {}\n",
    "    stage_name_dect = {}\n",
    "    submitted_stage_last_rdd_dect = {}\n",
    "    job_last_rdd_dect = {}\n",
    "    submitted_stages = set()\n",
    "    rdds_lst = []\n",
    "\n",
    "    def flush():\n",
    "        FactHub.app_name = \"\"\n",
    "        FactHub.job_info_dect = {}\n",
    "        FactHub.stage_info_dect = {}\n",
    "        FactHub.stage_job_dect = {}\n",
    "        FactHub.stage_name_dect = {}\n",
    "        FactHub.submitted_stage_last_rdd_dect = {}\n",
    "        FactHub.job_last_rdd_dect = {}\n",
    "        FactHub.submitted_stages.clear()\n",
    "        FactHub.rdds_lst = []\n",
    "\n",
    "        \n",
    "class AnalysisHub():\n",
    "    \n",
    "    transformations_set = set()\n",
    "    rdd_num_of_computations = defaultdict(int)\n",
    "    rdd_num_of_usage = defaultdict(int)\n",
    "    anomalies_dict = {}\n",
    "    stage_computed_rdds = {}\n",
    "    stage_used_rdds = {}\n",
    "    computed_rdds = set()\n",
    "    rdd_usage_lifetime_dict = {}\n",
    "    caching_plan_lst = []\n",
    "    memory_footprint_lst = []\n",
    "    cached_rdds_set = set()\n",
    "    non_cached_rdds_set = set()\n",
    "\n",
    "\n",
    "    def flush():\n",
    "        AnalysisHub.transformations_set.clear()\n",
    "        AnalysisHub.rdd_num_of_computations = defaultdict(int)\n",
    "        AnalysisHub.rdd_num_of_usage = defaultdict(int)\n",
    "        AnalysisHub.anomalies_dict = {}\n",
    "        AnalysisHub.stage_computed_rdds = {}\n",
    "        AnalysisHub.stage_used_rdds = {}\n",
    "        AnalysisHub.computed_rdds.clear()\n",
    "        AnalysisHub.rdd_usage_lifetime_dict = {}\n",
    "\n",
    "        \n",
    "class Parser():    \n",
    "    \n",
    "    def prepare(raw_log_file):\n",
    "        all_events_lst = pd.read_json(raw_log_file, lines=True)\n",
    "        FactHub.app_name = all_events_lst[all_events_lst['Event'] == 'SparkListenerApplicationStart']['App Name'].tolist()[0]\n",
    "        print(FactHub.app_name)\n",
    "        Parser.prepare_from_stage_submitted_events(all_events_lst[all_events_lst['Event'] == 'SparkListenerStageSubmitted'])\n",
    "        Parser.prepare_from_job_start_events(all_events_lst[all_events_lst['Event'] == 'SparkListenerJobStart'])\n",
    "        \n",
    "    def prepare_from_stage_submitted_events(stage_submitted_events):\n",
    "        for index, submitted_stage in enumerate(stage_submitted_events['Stage Info'].tolist()):\n",
    "            FactHub.submitted_stages.add(submitted_stage['Stage ID'])\n",
    "\n",
    "    def prepare_from_job_start_events(job_start_events):\n",
    "        job_ids_list = job_start_events['Job ID'].tolist()\n",
    "        job_stage_info_list = job_start_events['Stage Infos'].tolist()\n",
    "        for job_num, job_rec in enumerate(job_stage_info_list):\n",
    "            job_id = int(job_ids_list[job_num])\n",
    "            FactHub.job_info_dect[job_id] = job_rec\n",
    "            id_of_last_rdd_in_job = -1\n",
    "            for stage_num, stage_rec in enumerate(job_rec):\n",
    "                stage_id = int(stage_rec['Stage ID'])\n",
    "                FactHub.stage_job_dect[stage_id] = job_id\n",
    "                FactHub.stage_info_dect[stage_id] = stage_rec\n",
    "                FactHub.stage_name_dect[stage_id] = stage_rec['Stage Name']\n",
    "                id_of_last_rdd_in_stage = -1\n",
    "                for stage_rdd_num, stage_rdd_rec in enumerate(stage_rec['RDD Info']):\n",
    "                    rdd_id = stage_rdd_rec['RDD ID']\n",
    "                    is_cached = stage_rdd_rec['Storage Level']['Use Memory'] or stage_rdd_rec['Storage Level']['Use Disk']\n",
    "                    FactHub.rdds_lst.append(Rdd(rdd_id, stage_rdd_rec['Name'] + '\\n' + stage_rdd_rec['Callsite'], stage_rdd_rec['Parent IDs'], stage_id, job_id, is_cached))\n",
    "                    if id_of_last_rdd_in_job < rdd_id:\n",
    "                        id_of_last_rdd_in_job = rdd_id\n",
    "                    if id_of_last_rdd_in_stage < rdd_id:\n",
    "                        id_of_last_rdd_in_stage = rdd_id\n",
    "                if stage_id in FactHub.submitted_stages:\n",
    "                    FactHub.submitted_stage_last_rdd_dect[stage_id] = id_of_last_rdd_in_stage\n",
    "            FactHub.job_last_rdd_dect[job_id] = (id_of_last_rdd_in_job, stage_rec['Stage Name'])\n",
    "\n",
    "    \n",
    "class Analyzer():\n",
    "\n",
    "    def is_narrow_transformation(rdd_id, parent_id):\n",
    "        rdd_stages_set = set()\n",
    "        parent_stages_set = set()\n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            if rdd.id == rdd_id:\n",
    "                rdd_stages_set.add(rdd.stage_id)\n",
    "            elif rdd.id == parent_id:\n",
    "                parent_stages_set.add(rdd.stage_id)\n",
    "        return len(Utility.intersection(rdd_stages_set, parent_stages_set)) != 0\n",
    "\n",
    "    def prepare_transformations_lst():\n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            for parent_id in rdd.parents_lst:\n",
    "                AnalysisHub.transformations_set.add(Transformation(rdd.id, parent_id, Analyzer.is_narrow_transformation(rdd.id, parent_id)))\n",
    "\n",
    "    def add_rdd_and_its_parents_if_it_is_computed_in_stage(rdd_id, stage_id):#recursive\n",
    "        if rdd_id not in AnalysisHub.stage_used_rdds[stage_id]:\n",
    "            AnalysisHub.rdd_num_of_usage[rdd_id] += 1\n",
    "            AnalysisHub.stage_used_rdds[stage_id].add(rdd_id)            \n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            if rdd.id == rdd_id: \n",
    "                if rdd.is_cached:\n",
    "                    if rdd_id not in AnalysisHub.rdd_usage_lifetime_dict:\n",
    "                        AnalysisHub.rdd_usage_lifetime_dict[rdd.id] = (rdd.stage_id, rdd.job_id, rdd.stage_id, rdd.job_id)\n",
    "                    if AnalysisHub.rdd_usage_lifetime_dict[rdd_id][0] > stage_id:\n",
    "                        AnalysisHub.rdd_usage_lifetime_dict[rdd.id] = (rdd.stage_id, rdd.job_id, AnalysisHub.rdd_usage_lifetime_dict[rdd_id][2], AnalysisHub.rdd_usage_lifetime_dict[rdd_id][3])\n",
    "                    if AnalysisHub.rdd_usage_lifetime_dict[rdd_id][2] < stage_id:\n",
    "                        AnalysisHub.rdd_usage_lifetime_dict[rdd.id] = (AnalysisHub.rdd_usage_lifetime_dict[rdd_id][0], AnalysisHub.rdd_usage_lifetime_dict[rdd_id][1], rdd.stage_id, rdd.job_id)\n",
    "            if rdd.id == rdd_id: \n",
    "                if rdd.stage_id == stage_id:\n",
    "                    if rdd.is_cached:\n",
    "                        if rdd_id in AnalysisHub.computed_rdds: #already cached\n",
    "                            return\n",
    "                        AnalysisHub.computed_rdds.add(rdd_id) #cached for the first time\n",
    "                        AnalysisHub.stage_computed_rdds[stage_id].add(rdd_id)\n",
    "                    else:\n",
    "                        if rdd_id in AnalysisHub.computed_rdds: #handeling unpersistance\n",
    "                            AnalysisHub.computed_rdds.remove(rdd_id)\n",
    "                        AnalysisHub.stage_computed_rdds[stage_id].add(rdd_id)\n",
    "                    for parent_id in rdd.parents_lst:\n",
    "                        if Analyzer.is_narrow_transformation(rdd.id, parent_id):\n",
    "                            Analyzer.add_rdd_and_its_parents_if_it_is_computed_in_stage(parent_id, stage_id)\n",
    "\n",
    "    def calc_num_of_computations_of_rdds():\n",
    "        AnalysisHub.rdd_usage_lifetime_dict = {}\n",
    "        for stage_id in sorted(FactHub.submitted_stage_last_rdd_dect):\n",
    "            id_of_last_rdd_in_stage = FactHub.submitted_stage_last_rdd_dect[stage_id]\n",
    "            AnalysisHub.stage_computed_rdds[stage_id] = set()\n",
    "            AnalysisHub.stage_used_rdds[stage_id] = set()\n",
    "            Analyzer.add_rdd_and_its_parents_if_it_is_computed_in_stage(id_of_last_rdd_in_stage, stage_id)            \n",
    "        for stage_id in AnalysisHub.stage_computed_rdds:\n",
    "            for rdd_id in AnalysisHub.stage_computed_rdds[stage_id]:\n",
    "                AnalysisHub.rdd_num_of_computations[rdd_id] += 1\n",
    "\n",
    "    def prepare_anomalies_dict():\n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            rdd.name, rdd.is_cached, AnalysisHub.rdd_num_of_usage[rdd.id], AnalysisHub.rdd_num_of_computations[rdd.id]\n",
    "            if rdd.is_cached and AnalysisHub.rdd_num_of_usage[rdd.id] < int(config['Caching_Anomalies']['rdds_computation_tolerance_threshold']):\n",
    "                AnalysisHub.anomalies_dict[rdd.id] = \"unneeded cache\"\n",
    "            elif not rdd.is_cached and AnalysisHub.rdd_num_of_computations[rdd.id] >= int(config['Caching_Anomalies']['rdds_computation_tolerance_threshold']):\n",
    "                AnalysisHub.anomalies_dict[rdd.id] = \"recomputation\"\n",
    "\n",
    "    def prepare_caching_plan():\n",
    "        AnalysisHub.caching_plan_lst = []\n",
    "        for rdd_id, rdd_usage_lifetime in AnalysisHub.rdd_usage_lifetime_dict.items():\n",
    "            if config['Caching_Anomalies']['include_caching_anomalies_in_caching_plan'] == \"true\" or rdd_id not in AnalysisHub.anomalies_dict:\n",
    "                AnalysisHub.caching_plan_lst.append(CachingPlanItem(rdd_usage_lifetime[0], rdd_usage_lifetime[1], rdd_id, True))\n",
    "                AnalysisHub.caching_plan_lst.append(CachingPlanItem(rdd_usage_lifetime[2], rdd_usage_lifetime[3], rdd_id, False))        \n",
    "        AnalysisHub.memory_footprint_lst = []\n",
    "        incremental_rdds_set = set()\n",
    "        for caching_plan_item in sorted(AnalysisHub.caching_plan_lst):\n",
    "            if caching_plan_item.is_cache_item:\n",
    "                incremental_rdds_set.add(caching_plan_item.rdd_id)\n",
    "            else:\n",
    "                incremental_rdds_set.remove(caching_plan_item.rdd_id)\n",
    "            AnalysisHub.memory_footprint_lst.append((caching_plan_item.job_id, caching_plan_item.stage_id, (incremental_rdds_set.copy())))\n",
    "            \n",
    "    def analyze_caching_anomalies():\n",
    "        for rdd in FactHub.rdds_lst:\n",
    "            if rdd.id in AnalysisHub.cached_rdds_set:\n",
    "                rdd.is_cached = True\n",
    "            if rdd.id in AnalysisHub.non_cached_rdds_set:\n",
    "                rdd.is_cached = False\n",
    "        Analyzer.calc_num_of_computations_of_rdds()\n",
    "        Analyzer.prepare_anomalies_dict() \n",
    "        Analyzer.prepare_caching_plan() \n",
    "\n",
    "\n",
    "class SparkDataflowVisualizer():\n",
    "\n",
    "    def init():\n",
    "        AnalysisHub.cached_rdds_set.clear()\n",
    "        AnalysisHub.non_cached_rdds_set.clear()\n",
    "        FactHub.flush()\n",
    "        AnalysisHub.flush()\n",
    "    \n",
    "    def parse(raw_log_file):\n",
    "        Parser.prepare(raw_log_file)\n",
    "        \n",
    "    def analyze():\n",
    "        AnalysisHub.flush()\n",
    "        Analyzer.prepare_transformations_lst()\n",
    "        Analyzer.analyze_caching_anomalies()\n",
    "\n",
    "    def visualize_property_DAG():         \n",
    "        dot = graphviz.Digraph(strict=True, comment='Spark-Application-Graph', format = config['Output']['selected_format'])\n",
    "        dot.attr('node', shape=config['Drawing']['rdd_shape'], label='this is graph')\n",
    "        dot.node_attr={'shape': 'plaintext'}\n",
    "        dot.edge_attr.update(arrowhead='normal', arrowsize='1')\n",
    "        dag_rdds_set = set()\n",
    "        prev_action_name = \"\"\n",
    "        iterations_count = int(config['Drawing']['max_iterations_count']) \n",
    "        for job_id, job in sorted(FactHub.job_last_rdd_dect.items()):\n",
    "            action_name = job[1]\n",
    "            draw_iteration_indicator = False\n",
    "            if action_name == prev_action_name:\n",
    "                if iterations_count == 0:\n",
    "                    continue\n",
    "                iterations_count-=1\n",
    "            else:\n",
    "                iterations_count = int(config['Drawing']['max_iterations_count']) \n",
    "            for rdd in FactHub.rdds_lst:\n",
    "                if rdd.job_id == job_id and rdd.id not in dag_rdds_set:\n",
    "                    dag_rdds_set.add(rdd.id)\n",
    "                    node_label = \"\\n\"\n",
    "                    if config['Drawing']['show_action_id'] == \"true\":\n",
    "                        node_label = \"[\" + str(rdd.id) + \"] \" \n",
    "                    if config['Drawing']['show_rdd_name'] == \"true\":\n",
    "                        node_label = node_label + rdd.name[:int(config['Drawing']['rdd_name_max_number_of_chars'])]\n",
    "                    if config['Caching_Anomalies']['show_number_of_rdd_usage'] == \"true\":\n",
    "                        node_label = node_label + \"\\nused: \" + str(AnalysisHub.rdd_num_of_usage[rdd.id])\n",
    "                    if config['Caching_Anomalies']['show_number_of_rdd_computations'] == \"true\":\n",
    "                        node_label = node_label + \"\\ncomputed: \" + str(AnalysisHub.rdd_num_of_computations[rdd.id])\n",
    "                    if  config['Caching_Anomalies']['highlight_unneeded_cached_rdds'] == \"true\" and AnalysisHub.anomalies_dict.get(rdd.id, \"\") == \"unneeded cache\":\n",
    "                        dot.node(str(rdd.id), penwidth = '3', fillcolor = config['Drawing']['cached_rdd_bg_color'], color = 'red', shape = config['Drawing']['anomaly_shape'], style = 'filled', label = node_label)\n",
    "                    elif config['Caching_Anomalies']['highlight_recomputed_rdds'] == \"true\" and AnalysisHub.anomalies_dict.get(rdd.id, \"\") == \"recomputation\":\n",
    "                        dot.node(str(rdd.id), penwidth = '3', fillcolor = 'white', color = 'red', shape = config['Drawing']['anomaly_shape'], style = 'filled', label = node_label)\n",
    "                    else:\n",
    "                        dot.node(str(rdd.id), fillcolor = config['Drawing']['cached_rdd_bg_color'] if rdd.is_cached else 'white', style = 'filled', label = node_label)\n",
    "            action_lable = \"\" \n",
    "            if config['Drawing']['show_action_id'] == \"true\":\n",
    "                action_lable = \"[\" + str(job_id) + \"]\"\n",
    "            if config['Drawing']['show_action_name'] == \"true\":\n",
    "                action_lable = action_lable + action_name[:int(config['Drawing']['action_name_max_number_of_chars'])]\n",
    "            \n",
    "            if draw_iteration_indicator == True:    \n",
    "                draw_iteration_indicator = False\n",
    "                continue\n",
    "            dot.node(\"Action_\" + str(job_id), shape=config['Drawing']['action_shape'] if iterations_count != 0 else config['Drawing']['iterative_action_shape'], fillcolor = config['Drawing']['action_bg_collor'] if iterations_count != 0 else config['Drawing']['iterative_action_collor'], style = 'filled', label = action_lable)\n",
    "            dot.edge(str(job[0]), \"Action_\" + str(job_id), color = 'black', arrowhead = 'none', style = 'dashed')\n",
    "            prev_action_name = action_name\n",
    "        for transformation in sorted(AnalysisHub.transformations_set):\n",
    "            if transformation.to_rdd in dag_rdds_set and transformation.from_rdd in dag_rdds_set:\n",
    "                dot.edge(str(transformation.to_rdd), str(transformation.from_rdd), color = config['Drawing']['narrow_transformation_color'] if transformation.is_narrow else config['Drawing']['wide_transformation_color'])        \n",
    "        caching_plan_label = \"\\nRecommended Schedule:\\n\"\n",
    "        for caching_plan_item in sorted(AnalysisHub.caching_plan_lst):\n",
    "            if caching_plan_item.is_cache_item:\n",
    "                caching_plan_label += \"\\nCache \"\n",
    "            else:\n",
    "                caching_plan_label += \"\\nUnpersist \"\n",
    "            caching_plan_label += \"RDD[\" + str(caching_plan_item.rdd_id) + \"] \" + (\"at\" if caching_plan_item.is_cache_item else \"after\") + \" stage(\" + str(caching_plan_item.stage_id) + \") in job(\" + str(caching_plan_item.job_id) + \")\\n\"\n",
    "        caching_plan_label += \"\\n\"\n",
    "        if len(AnalysisHub.caching_plan_lst) > 0 and config['Caching_Anomalies']['show_caching_plan'] == \"true\":\n",
    "            dot.node(\"caching_plan\", shape = 'note', fillcolor = 'lightgray', style = 'filled', label = caching_plan_label)\n",
    "        memory_footprint_label = \"\\nMemory Footprint:\\n\"\n",
    "        for memory_footprint_item in AnalysisHub.memory_footprint_lst:\n",
    "            memory_footprint_label += \"\\n\"\n",
    "            if len(memory_footprint_item[2]) == 0:\n",
    "                memory_footprint_label += \"Free\"\n",
    "            else:\n",
    "                memory_footprint_label += str(memory_footprint_item[2])\n",
    "            memory_footprint_label += \"\\n\"\n",
    "        memory_footprint_label += \"\\n\"\n",
    "        if len(AnalysisHub.caching_plan_lst) > 0 and config['Caching_Anomalies']['show_memory_footprint'] == \"true\":\n",
    "            dot.node(\"memory_footprint\", shape = 'note', fillcolor = 'lightgray', style = 'filled', label = memory_footprint_label)\n",
    "        dot.attr(labelloc=\"t\")\n",
    "        dot.attr(label=FactHub.app_name)\n",
    "        dot.attr(fontsize='40')\n",
    "        spark_dataflow_visualizer_output_path = Utility.get_absolute_path(config['Paths']['output_path'])\n",
    "        output_file_name = re.sub('[^a-zA-Z0-9]+', '', FactHub.app_name)\n",
    "        dot.render(spark_dataflow_visualizer_output_path + '/' + output_file_name, view=config['Output']['view_after_render'] == 'true')\n",
    "        \n",
    "\n",
    "# Useful functions for the demonstration \n",
    "\n",
    "def load_file(file_name):\n",
    "    spark_dataflow_visualizer_input_path = Utility.get_absolute_path(config['Paths']['input_path'])\n",
    "    log_file_path = spark_dataflow_visualizer_input_path + '/' + file_name\n",
    "    SparkDataflowVisualizer.init()\n",
    "    SparkDataflowVisualizer.parse(log_file_path)\n",
    "\n",
    "def draw_DAG():\n",
    "    SparkDataflowVisualizer.analyze()\n",
    "    SparkDataflowVisualizer.visualize_property_DAG()\n",
    "    \n",
    "def cache(rdd_id):\n",
    "    AnalysisHub.cached_rdds_set.add(rdd_id)\n",
    "    AnalysisHub.non_cached_rdds_set.discard(rdd_id)\n",
    "    draw_DAG()\n",
    "    \n",
    "def dont_cache(rdd_id):\n",
    "    AnalysisHub.non_cached_rdds_set.add(rdd_id)\n",
    "    AnalysisHub.cached_rdds_set.discard(rdd_id)\n",
    "    draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32639513",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file('application_1641567765635_0122')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5846a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file('application_1641567765635_0023')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a589d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file('application_1635092038229_0122')\n",
    "config.read('config.ini')\n",
    "config['Caching_Anomalies']['highlight_recomputed_rdds'] = 'true'\n",
    "config['Caching_Anomalies']['highlight_unneeded_cached_rdds'] = 'true'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "ca68dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache(31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b9345638",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_cache(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "27bf3f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_cache(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe1c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file('application_1641567765635_0161')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "52b21165",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache(35)\n",
    "cache(53)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a521c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file('local-1641586266617')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50e9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file('application_1635092038229_0130')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d52dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file('application_1635092038229_0126')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2086e7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file('application_1635092038229_0144')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addb77b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file('application_1635092038229_0140')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "94c7e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Drawing']['max_iterations_count'] = '5'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "b66c913e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Caching_Anomalies']['rdds_computation_tolerance_threshold'] = '4'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5195fc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_cache(217)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f8c4d448",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "40e9bef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dont_cache(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "40dd1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f404aa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d76f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_file('application_1635092038229_0124')\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9d7556e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Drawing']['max_iterations_count'] = '5'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "988a0cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Caching_Anomalies']['rdds_computation_tolerance_threshold'] = '3'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9de7e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "config['Caching_Anomalies']['rdds_computation_tolerance_threshold'] = '4'\n",
    "draw_DAG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c82ba0a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8438ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.read('config.ini')\n",
    "config['Output']['view_after_render'] = 'false'\n",
    "\n",
    "file_list = os.listdir(config['Paths']['input_path'])\n",
    "for os_file_name in file_list:\n",
    "    load_file(os_file_name)\n",
    "    draw_DAG()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
